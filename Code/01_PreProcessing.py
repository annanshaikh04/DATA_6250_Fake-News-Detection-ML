# -*- coding: utf-8 -*-
"""PreProcessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18rdBW9YbiChUT4AyZkojEH_lM3gpJK5p

# Fake News Classification – Preprocessing Notebook

### Goal
The goal of this notebook is to clean, process, and prepare the dataset for machine learning models to classify fake and real news articles. We will handle missing data, clean text fields, remove stopwords, and generate numerical embeddings to feed into ML classifiers.

## 4.1 Dataset Analysis – Step 1: Load Required Libraries and Dataset Files
"""

# Mount Google Drive to access datasets

from google.colab import drive
drive.mount('/content/drive')

# Import essential libraries
import pandas as pd
import numpy as np
import re
import matplotlib.pyplot as plt
import seaborn as sns
from nltk.corpus import stopwords
import os

# Load train, test, and submission datasets from drive
train_data = pd.read_csv('/content/drive/MyDrive/Annan Project/Datasets/train.csv')
test_data = pd.read_csv('/content/drive/MyDrive/Annan Project/Datasets/test.csv')
submit_data = pd.read_csv('/content/drive/MyDrive/Annan Project/Datasets/submit.csv')

# Preview the first few rows of training data
train_data.head()

# Preview the test data
test_data.head()

"""## 4.1 Dataset Summary and Feature Information

### Dataset Details:
- Dataset Source: Kaggle Fake News Dataset (Competition Dataset)

| Dataset | Rows | Description |
|---------|------|-------------|
| train.csv | 20,800 | Contains labeled data (Real/Fake) |
| test.csv  | 5,200  | Unlabeled data (Model will predict) |
| submit.csv | 5,200  | Submission file for competition |

---

### Columns Description:
| Column | Description |
|--------|-------------|
| id | Unique ID for each news article |
| title | Title of the article |
| author | Name of the author |
| text | Full content of the article |
| label | Target variable (1 = Fake, 0 = Real) |

## Why This Dataset Was Chosen?

- It is highly relevant due to rising misinformation problems.
- Sufficient size for training Machine Learning models.
- Multiple textual features (title, text, author) available.
- Real-world application for news/media industry.

## Strengths & Weaknesses of Dataset:

### Strengths:
- Sufficient size & labeled data
- Real-world problem
- Good feature diversity (title, text, author)

### Weaknesses:
- Missing values in `author` column
- Noisy text data
- Class imbalance

## Dataset Bias Analysis:

### Potential Biases:
- Certain authors or headlines may dominate class prediction.
- Source or style-based bias possible.
- Class imbalance (slightly higher fake news samples).

### Impact of Bias:
- Model may overfit to stylistic patterns.
- Misclassification risk increases if not handled properly.

## 4.2 Preprocessing Steps — Step 1: Handling Missing Values

→ We found missing values in the `author` column.  
To handle this, we filled missing author names with `Unknown`.
"""

# Fill missing values in 'author' with 'Unknown'
train_data['author'].fillna('Unknown', inplace=True)
test_data['author'].fillna('Unknown', inplace=True)

"""## Step 2: Text Cleaning (Removing Special Characters, Numbers & Lowercasing)

→ Text cleaning helps in reducing noise from data and making it uniform for model input.
"""

# Function to clean text columns
def clean_text(text):
    text = re.sub(r'\W', ' ', str(text))  # Remove special characters
    text = re.sub(r'\d+', ' ', text)  # Remove numbers
    text = text.lower()  # Convert text to lowercase
    return text

# Apply cleaning to both 'title' and 'text'
train_data['title'] = train_data['title'].apply(clean_text)
train_data['text'] = train_data['text'].apply(clean_text)
test_data['title'] = test_data['title'].apply(clean_text)
test_data['text'] = test_data['text'].apply(clean_text)

"""## Step 3: Remove Stopwords (Common Irrelevant Words)

→ Words like 'the', 'is', 'in' etc., do not carry much meaning for model learning.
"""

# Import and Download NLTK stopwords
import nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')

stop_words = set(stopwords.words('english'))

# Function to remove stopwords
def remove_stopwords(text):
    return ' '.join(word for word in text.split() if word not in stop_words)

# Apply stopword removal
train_data['text'] = train_data['text'].apply(remove_stopwords)
test_data['text'] = test_data['text'].apply(remove_stopwords)

"""## Step 4: Check Class Balance (Distribution of Fake vs Real News)

→ Important to check if data is balanced for both labels.

## Class Balance Check

It’s important to confirm whether the dataset is balanced between real and fake news before training the model.

This impacts both model performance and evaluation.

If the dataset is **imbalanced**, we apply undersampling to avoid bias.
"""

# Define a function to check label distribution
def check_balance(data):
  pos = len(data[data['label'] == 0])
  neg = len(data[data['label'] == 1])
  if pos == neg:
    print('Dataset is balanced')
  else:
    print('Dataset is not balanced')

# Apply balance check on training data
check_balance(train_data)

# Print actual counts of both classes
print(len(train_data[train_data['label'] == 0]))
print(len(train_data[train_data['label'] == 1]))

"""## Step 6: Balance the Dataset (Undersampling)

Since the dataset is **not perfectly balanced**, we perform undersampling to equalize both classes.

We randomly sample equal-sized data from each class.

"""

# Perform undersampling to balance dataset
min_count = min(
    len(train_data[train_data['label'] == 0]),
    len(train_data[train_data['label'] == 1])
)
# Sample equal records from both classes
class_0 = train_data[train_data['label'] == 0].sample(n=min_count, random_state=42)
class_1 = train_data[train_data['label'] == 1].sample(n=min_count, random_state=42)

# Combine and shuffle the balanced data
balanced_data = pd.concat([class_0, class_1]).sample(frac=1, random_state=42).reset_index(drop=True)

"""## Step 7: Sentence Embedding using Pretrained Transformer Models

We now move toward embedding-based feature engineering using the Sentence Transformer library.

We use:
- **`all-mpnet-base-v2`** → to evaluate semantic similarity for label prediction (in missing author name logic)
- **`all-MiniLM-L6-v2`** → to generate dense numerical features for model training

### Step 7.1: Install Required Libraries
"""

# Install transformer and PyTorch libraries
!pip install -U sentence-transformers torch

"""### Step 7.2: Import Required Libraries"""

import torch
from sentence_transformers import SentenceTransformer, util

"""## Step 8: Predicting Author Label Confidence using Sentence Embeddings

Since many `author` names are missing or unhelpful (filled as 'Unknown'),  
we are using a smart approach by leveraging Sentence Transformers to calculate how much the content of an article looks like:

- "Fake" → `Fake_Confidence` score
- "Real" → `Real_Confidence` score

---

### Models Used:
| Model | Purpose |
|-------|---------|
| all-mpnet-base-v2 | To generate label confidence (Real/Fake) |
| all-MiniLM-L6-v2 | To generate numerical features from text for ML models |

"""

# Import tqdm for progress visualization
from tqdm import tqdm
tqdm.pandas()  # Adding progress bar support for pandas apply

"""## Initialize the Sentence Transformer Models"""

# Load the sentence transformer models
label_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')
feature_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')

"""## Generate Static Embeddings for Target Labels
This helps us later to calculate cosine similarity between article content and label embeddings.
"""

# Generate embeddings for both labels "Fake" and "Real"
fake_emb = label_model.encode('Fake',convert_to_tensor=True)
real_emb = label_model.encode('Real',convert_to_tensor=True)

"""## Validate Data Columns Before Processing"""

# Print all column names of balanced_data for reference
for col in balanced_data.columns:
  print(repr(col))

"""## Generate Confidence Scores using Cosine Similarity
This function generates:
- `Fake_Confidence` → Similarity of article to word "Fake"
- `Real_Confidence` → Similarity of article to word "Real"
"""

# Define function to calculate label confidence
def get_label_confidence(row_vals):
  text = (f"Title: {row_vals['title']}. Text: {row_vals['text']} Published by: {row_vals['author']}")

  # Convert text into embeddings
  text_emb = label_model.encode(text, convert_to_tensor=True)

  # Calculate cosine similarity with fake and real embeddings
  cos_sim_fake = util.cos_sim(text_emb, fake_emb)
  cos_sim_real = util.cos_sim(text_emb, real_emb)

  return cos_sim_fake.item(), cos_sim_real.item()

# Apply function to balanced dataset
balanced_data['Real_Confidence'], balanced_data['Fake_Confidence'] = zip(*balanced_data.progress_apply(get_label_confidence, axis=1))

"""## View Sample Records with Generated Label Confidences"""

balanced_data.head()

"""## Step 9: Generate Final Feature Embeddings using MiniLM Model

### Purpose:
We now create dense numerical features for both training and test data using the Sentence Transformer model `all-MiniLM-L6-v2`.

This helps in converting the entire text into numbers for feeding into ML models.

---

### Strategy:
- Combine `title`, `text`, and `author` fields.
- Generate embeddings for each record.
- Store embeddings as multiple features (feature_0, feature_1, ..., feature_n)

"""

# Make a copy of balanced dataset for feature creation
df_new = balanced_data.copy()

# Define function to generate embeddings using MiniLM
def get_features(row_vals):
  text = (f"For the given news: " +
          f"Title: {row_vals['title']}, " +
          f"The news is: {row_vals['text']}, " +
          f"which is published by: {row_vals['author']}")

  # Generate embeddings
  text_emb = feature_model.encode(text, convert_to_tensor=True)

  # Convert embeddings to numpy array
  if hasattr(text_emb,'cpu'):
    text_emb = text_emb.cpu().detach().numpy()
  else:
    text_emb = text_emb.numpy()

  # Flatten if needed
  if text_emb.ndim > 1:
    text_emb = text_emb.flatten()

  # Store embeddings as new features
  for i, value in enumerate(text_emb):
        row_vals[f'feature_{i}'] = value
  return row_vals

"""## Apply Embedding Function to Training Data"""

df_new = df_new.progress_apply(get_features, axis=1)

"""## Drop Unnecessary Columns: title, text, author"""

df_new.drop(columns=[col for col in ['title','text','author'] if col in df_new.columns], axis=1, inplace=True)

"""## Reorder Columns: Features First, Confidence Scores & Labels Last"""

right_order = ['Real_Confidence','Fake_Confidence','label']
left_order = [col for col in df_new.columns if col not in right_order]
df_new = df_new[left_order + right_order]
df_new.head()

"""## Export Final Training Data with Generated Features"""

train_path = '/content/drive/MyDrive/Annan Project/Datasets/Feature Converted/train.csv'

# Remove existing file if exists
if os.path.exists(train_path):
    os.remove(train_path)

# Create path if not exists
path = os.path.dirname(train_path)
if not os.path.exists(path):
    os.makedirs(path)
# Save file
df_new.to_csv(train_path, index=False)

"""## Apply Same Process to Test Data"""

test_data.head()

"""### Generate Real & Fake Confidence for Test Data"""

test_data['Real_Confidence'], test_data['Fake_Confidence'] = zip(*test_data.progress_apply(get_label_confidence, axis=1))

"""### Generate Embeddings for Test Data using MiniLM"""

test_data = test_data.progress_apply(get_features, axis=1)

"""## Step 10: Preparing Final Test Data

Now that we have generated both:
- `Real_Confidence` and `Fake_Confidence`
- Embeddings using MiniLM model

We now clean the test data, reorder the columns, and export it as final clean test data.

---

### Columns Arrangement:
1. Feature Columns first
2. `Real_Confidence` and `Fake_Confidence` at the end
"""

# Reorder columns of test data
right_order = ['Real_Confidence','Fake_Confidence']
left_order = [col for col in test_data.columns if col not in right_order]
test_data = test_data[left_order + right_order]

# Drop unnecessary columns: title, text, author
test_data.drop(columns=[col for col in ['title','text','author'] if col in test_data.columns], axis=1, inplace=True)

# Preview final structure of test_data
test_data.head()

"""## Export Final Test Dataset to Drive"""

# Define path to save test data
train_path = '/content/drive/MyDrive/Annan Project/Datasets/Feature Converted/test.csv'

# Remove existing test file if it exists
if os.path.exists(train_path):
    os.remove(train_path)

# Create folder path if not exists
path = os.path.dirname(train_path)
if not os.path.exists(path):
    os.makedirs(path)
# Save final clean test data
df_new.to_csv(train_path, index=False)
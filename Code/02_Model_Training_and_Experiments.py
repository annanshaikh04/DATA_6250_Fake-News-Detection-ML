# -*- coding: utf-8 -*-
"""ML_Proj.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tCQG6P1d395ctIWZ_uQC-qogJTB0EudF

# Fake News Classification using ML Models

## Notebook Purpose:
This notebook focuses on:
- Training multiple machine learning models.
- Evaluating their performance.
- Performing dimensionality reduction for feature optimization.
- Comparing models for final deployment readiness.

---

## Loading Required Libraries and Datasets
"""

# Mounting Google Drive to access data files stored in Drive
from google.colab import drive
drive.mount('/content/drive')

# Importing necessary Python libraries for Data Handling, Visualization and Preprocessing
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
from tqdm import tqdm
import warnings
warnings.filterwarnings("ignore")

from sklearn.preprocessing import StandardScaler

"""## Loading Preprocessed Train and Test Data
(Exported from PreProcessing Notebook)
"""

# Load Cleaned Train and Test Data
input_path = "/content/drive/MyDrive/Annan Project/Datasets/Feature Converted/"
train_df = pd.read_csv(input_path + "train.csv")
test_df = pd.read_csv(input_path + "test.csv")

# View Train and Test Columns
train_df.columns

test_df.columns

"""## Dimensionality Reduction using UMAP
- Reason: Embedding features from PreProcessing are high-dimensional.
- UMAP reduces dimensions while preserving structure.
"""

# Installing UMAP package
!pip install umap-learn

# UMAP for Dimensionality Reduction
import umap

# Select Feature Columns from feature_0 to feature_383
features = [f"feature_{i}" for i in range(0, 384)]

# Extract Features
X_train = train_df[features]
X_test = test_df[features]

# Standardize Features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Apply UMAP to reduce to 80 components
reducer = umap.UMAP(n_components=80,
                    min_dist=0.0,
                    metric='cosine',
                    random_state=42) # Adjust n_components as needed
X_train_umap = reducer.fit_transform(X_train_scaled)
X_test_umap = reducer.transform(X_test_scaled)

# Add UMAP Reduced Components to DataFrame
for i in tqdm(range(80)):
    train_df[f'reduced_component_{i}'] = X_train_umap[:, i]
    test_df[f'reduced_component_{i}'] = X_test_umap[:, i]

# Dropping original feature columns (since reduced components are created)
train_df = train_df.drop(columns=features)
test_df = test_df.drop(columns=features)

"""## Saving Data After UMAP Reduction (Before Model Experimentation)"""

# Exporting Reduced Train and Test Data
output_path_large_models = "/content/drive/MyDrive/Annan Project/Datasets/ML Project/Before_Experiment_train.csv"

# Remove if exists
if os.path.exists(output_path_large_models):
    os.remove(output_path_large_models)
path = os.path.dirname(output_path_large_models)

# Create directories if not exist
if not os.path.exists(path):
    os.makedirs(path)
train_df.to_csv(output_path_large_models, index=False)

output_path_large_models = "/content/drive/MyDrive/Annan Project/Datasets/ML Project/Before_Experiment_test.csv"

# Remove if exists
if os.path.exists(output_path_large_models):
    os.remove(output_path_large_models)
path = os.path.dirname(output_path_large_models)

# Create directories if not exist
if not os.path.exists(path):
    os.makedirs(path)
test_df.to_csv(output_path_large_models, index=False)

# View Final Columns
train_df.columns

test_df.columns

"""## Selecting Final Columns for Model Training
(Including UMAP Components + Real/Fake Confidence Scores)
"""

num_cols = [f"reduced_component_{i}" for i in range(80)] + ['Real_Confidence','Fake_Confidence']

# Scaling Final Selected Columns
scaler = StandardScaler()
train_df[num_cols] = scaler.fit_transform(train_df[num_cols])
test_df[num_cols] = scaler.transform(test_df[num_cols])

"""## Model Training — Logistic Regression
(Logistic Regression is selected as first baseline model to compare performance)
"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Define features (X) and target (y)
X_train = train_df[num_cols]
y_train = train_df['label']

X_val = test_df[num_cols]
y_val = test_df['label']

# Train Logistic Regression
logreg_model = LogisticRegression()
logreg_model.fit(X_train, y_train)

# Predict on Validation
y_pred = logreg_model.predict(X_val)

# Evaluate Accuracy
accuracy = accuracy_score(y_val, y_pred)
print(f"Validation Accuracy: {accuracy}")

"""### Logistic Regression – Evaluation

Let's evaluate our baseline model using:
- Confusion Matrix
- Precision, Recall, F1-Score
"""

# Plot Confusion Matrix for Logistic Regression
from sklearn.metrics import ConfusionMatrixDisplay
titles_options = [
    ("Confusion matrix, without normalization", None),
    ("Normalized confusion matrix", 'true'),
]

for title, normalize in titles_options:
    disp = ConfusionMatrixDisplay.from_estimator(
        logreg_model,
        X_val,
        y_val,
        display_labels=logreg_model.classes_,
        cmap=plt.cm.Blues,
        normalize=normalize,
    )
    disp.ax_.set_title(title)
    print(title)
    print(disp.confusion_matrix)
    plt.show()

# Precision, Recall, F1-Score Plot for Logistic Regression
from sklearn.metrics import classification_report

report = classification_report(y_val, y_pred, output_dict=True)
report_df = pd.DataFrame(report).transpose()

# Bar chart
plt.figure(figsize=(8, 5))
report_df.iloc[:-1, :3].plot(kind='bar')  # Avoid 'accuracy' row
plt.title('Logistic Regression: Precision, Recall & F1-Score')
plt.ylabel('Score')
plt.ylim(0, 1.05)
plt.grid(True)
plt.tight_layout()
plt.show()

"""---

### Support Vector Machines (SVM)

I will compare 3 kernels:
- Linear
- Polynomial (Degree = 3)
- RBF

All trained on scaled UMAP + confidence features.

"""

# Train & Evaluate SVM Classifier on 3 Kernels
from sklearn.svm import SVC

# Linear Kernel
svm_linear = SVC(kernel='linear')
svm_linear.fit(X_train, y_train)
y_pred_linear = svm_linear.predict(X_val)
accuracy_linear = accuracy_score(y_val, y_pred_linear)
print(f"Linear Kernel Accuracy: {accuracy_linear}")

# Polynomial Kernel
svm_poly = SVC(kernel='poly', degree=3) # You can adjust the degree
svm_poly.fit(X_train, y_train)
y_pred_poly = svm_poly.predict(X_val)
accuracy_poly = accuracy_score(y_val, y_pred_poly)
print(f"Polynomial Kernel Accuracy: {accuracy_poly}")

# RBF Kernel
svm_rbf = SVC(kernel='rbf') # You can adjust parameters like C and gamma
svm_rbf.fit(X_train, y_train)
y_pred_rbf = svm_rbf.predict(X_val)
accuracy_rbf = accuracy_score(y_val, y_pred_rbf)
print(f"RBF Kernel Accuracy: {accuracy_rbf}")

# Bar Chart to Compare Accuracy Across Kernels
import matplotlib.pyplot as plt

# Prepare data
kernels = ['Linear Kernel', 'Polynomial Kernel', 'RBF Kernel']
accuracies = [accuracy_linear, accuracy_poly, accuracy_rbf]

# Plot
plt.figure(figsize=(7,5))
plt.bar(kernels, accuracies, color=['blue', 'green', 'orange'])
plt.ylabel('Accuracy Score')
plt.title('SVM Kernel Comparison - Accuracy')
plt.ylim(0, 1)
plt.grid(True)
plt.tight_layout()
plt.show()

# Confusion Matrices for Each SVM Model
from sklearn.metrics import ConfusionMatrixDisplay

models = {
    'Linear Kernel': svm_linear,
    'Polynomial Kernel': svm_poly,
    'RBF Kernel': svm_rbf
}

predictions = {
    'Linear Kernel': y_pred_linear,
    'Polynomial Kernel': y_pred_poly,
    'RBF Kernel': y_pred_rbf
}

# Plot Confusion Matrix for each Kernel
for name in models.keys():
    print(f'Confusion Matrix for {name}')
    disp = ConfusionMatrixDisplay.from_estimator(
        models[name],
        X_val,
        y_val,
        cmap=plt.cm.Blues,
        normalize='true'  # Normalized for better visual
    )
    disp.ax_.set_title(f'{name} Confusion Matrix')
    plt.show()

# Classification Report Chart for All SVM Kernels
from sklearn.metrics import classification_report
import pandas as pd

# For Each Kernel
for name, y_pred in predictions.items():
    print(f'Classification Report for {name}')
    report = classification_report(y_val, y_pred, output_dict=True)
    report_df = pd.DataFrame(report).transpose()

    report_df.iloc[:-1, :3].plot(kind='bar', figsize=(8,5))
    plt.title(f'{name} : Precision, Recall, F1-Score')
    plt.ylabel('Score')
    plt.ylim(0, 1.05)
    plt.grid(True)
    plt.tight_layout()
    plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.svm import SVC
from sklearn.datasets import make_blobs
from sklearn.preprocessing import StandardScaler


# Generate sample data
X, y = make_blobs(n_samples=100, centers=2, random_state=42, cluster_std=1.5)
X = StandardScaler().fit_transform(X)


# Create and train an SVM classifier
svm_classifier = SVC(kernel='linear', C=1)
svm_classifier.fit(X, y)

# Create the plot
plt.figure(figsize=(8, 6))

# Plot the decision boundary
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),
                     np.arange(y_min, y_max, 0.1))
Z = svm_classifier.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
plt.contourf(xx, yy, Z, alpha=0.4)

# Plot the data points
plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k')

# Add labels and title
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.title("SVM Decision Boundary")

# Add a legend
plt.legend(["Class 0", "Class 1"]) # Replace with your actual class names
plt.show()

"""---

### Gradient Boosting Classifier
"""

from sklearn.ensemble import GradientBoostingClassifier

# Train Gradient Boosting Classifier
gb_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42) # Adjust hyperparameters as needed
gb_model.fit(X_train, y_train)

# Make predictions on the validation set
y_pred_gb = gb_model.predict(X_val)

# Evaluate the model
accuracy_gb = accuracy_score(y_val, y_pred_gb)
print(f"Gradient Boosting Accuracy: {accuracy_gb}")

# Gradient Boosting: Confusion Matrix
from sklearn.metrics import ConfusionMatrixDisplay, classification_report
import matplotlib.pyplot as plt

# Plot Confusion Matrix
titles_options = [
    ("Confusion matrix, without normalization", None),
    ("Normalized confusion matrix", 'true'),
]

for title, normalize in titles_options:
    disp = ConfusionMatrixDisplay.from_estimator(
        gb_model,
        X_val,
        y_val,
        cmap=plt.cm.Blues,
        normalize=normalize,
    )
    disp.ax_.set_title(title + " - Gradient Boosting")
    print(title)
    print(disp.confusion_matrix)
    plt.show()

# Gradient Boosting: Classification Report
import pandas as pd

# Classification Report
report_gb = classification_report(y_val, y_pred_gb, output_dict=True)
report_gb_df = pd.DataFrame(report_gb).transpose()

# Plot
report_gb_df.iloc[:-1, :3].plot(kind='bar', figsize=(8,5))
plt.title('Gradient Boosting: Precision, Recall, F1-Score')
plt.ylabel('Score')
plt.ylim(0, 1.05)
plt.grid(True)
plt.tight_layout()
plt.show()

#   Gradient Boosting: Feature Importance Plot
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Step 1: Get feature importances from the trained GB model
importances = gb_model.feature_importances_

# Step 2: Create Series with feature names
gb_importances = pd.Series(importances, index=num_cols)

# Step 3: Sort and pick Top 20 Important Features
top_20_features = gb_importances.sort_values(ascending=False).head(20)

# Step 4: Plot
plt.figure(figsize=(12, 6))
top_20_features.plot(kind='barh')
plt.gca().invert_yaxis()
plt.xlabel('Feature Importance Score')
plt.title('Top 20 Most Important Features - Gradient Boosting')
plt.tight_layout()
plt.show()

# Step 5: Print Top 20 (Optional for report)
print("Top 20 Features by Importance in Gradient Boosting:\n")
print(top_20_features)

"""---

## Random Forest Classifier – Training & Evaluation

"""

from sklearn.ensemble import RandomForestClassifier

# Initialize and train the Random Forest Classifier
rf_model = RandomForestClassifier(n_estimators=100, random_state=42) # Adjust hyperparameters as needed
rf_model.fit(X_train, y_train)

# Predict on validation data
y_pred_rf = rf_model.predict(X_val)

# Evaluate accuracy
accuracy_rf = accuracy_score(y_val, y_pred_rf)
print(f"Random Forest Accuracy: {accuracy_rf}")

"""### Random Forest: Confusion Matrix"""

from sklearn.metrics import ConfusionMatrixDisplay, classification_report
import matplotlib.pyplot as plt

# Plot confusion matrices for Random Forest (normalized and raw)
titles_options = [
    ("Confusion matrix, without normalization", None),
    ("Normalized confusion matrix", 'true'),
]

for title, normalize in titles_options:
    disp = ConfusionMatrixDisplay.from_estimator(
        rf_model,
        X_val,
        y_val,
        display_labels=rf_model.classes_,
        cmap=plt.cm.Blues,
        normalize=normalize,
    )
    disp.ax_.set_title(title + " - Random Forest")
    print(title)
    print(disp.confusion_matrix)
    plt.show()

import pandas as pd

# Classification Report for Random Forest
report_rf = classification_report(y_val, y_pred_rf, output_dict=True)
report_rf_df = pd.DataFrame(report_rf).transpose()

report_rf_df.iloc[:-1, :3].plot(kind='bar', figsize=(8,5))
plt.title('Random Forest: Precision, Recall, F1-Score')
plt.ylabel('Score')
plt.ylim(0, 1.05)
plt.grid(True)
plt.tight_layout()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# Feature Names
feature_names = num_cols  # This should have your feature column names

# Get Feature Importances from Model
importances = rf_model.feature_importances_

# Create DataFrame with Feature Names & Importance Score
forest_importances = pd.DataFrame({
    'Feature': feature_names,
    'Importance': importances
})

# Sort and Select Top 20 Features
top_20_features = forest_importances.sort_values(by='Importance', ascending=False).head(20)

# Plot Top 20 Features
plt.figure(figsize=(12, 6))
plt.barh(top_20_features['Feature'], top_20_features['Importance'])
plt.xlabel('Feature Importance Score')
plt.title('Top 20 Most Important Features - Random Forest')
plt.gca().invert_yaxis()  # To show the most important feature at the top
plt.tight_layout()
plt.show()

# Print Top 20 Features for Report
print(top_20_features)

"""---

## Decision Tree Classifier - Evaluation
"""

from sklearn.tree import DecisionTreeClassifier

# Initialize and train the Decision Tree Classifier
dt_model = DecisionTreeClassifier(random_state=42) # Adjust hyperparameters as needed
dt_model.fit(X_train, y_train)

# Predict on validation set
y_pred_dt = dt_model.predict(X_val)

# Evaluate Accuracy
accuracy_dt = accuracy_score(y_val, y_pred_dt)
print(f"Decision Tree Accuracy: {accuracy_dt}")

"""### Decision Tree: Confusion Matrix"""

from sklearn.metrics import ConfusionMatrixDisplay, classification_report
import matplotlib.pyplot as plt

# Plot Confusion Matrix for Decision Tree
titles_options = [
    ("Confusion matrix, without normalization", None),
    ("Normalized confusion matrix", 'true'),
]

for title, normalize in titles_options:
    disp = ConfusionMatrixDisplay.from_estimator(
        dt_model,
        X_val,
        y_val,
        cmap=plt.cm.Blues,
        normalize=normalize,
    )
    disp.ax_.set_title(title + " - Decision Tree")
    print(title)
    print(disp.confusion_matrix)
    plt.show()

import pandas as pd

# Classification Report
report_dt = classification_report(y_val, y_pred_dt, output_dict=True)
report_dt_df = pd.DataFrame(report_dt).transpose()

# Plot
report_dt_df.iloc[:-1, :3].plot(kind='bar', figsize=(8,5))
plt.title('Decision Tree: Precision, Recall, F1-Score')
plt.ylabel('Score')
plt.ylim(0, 1.05)
plt.grid(True)
plt.tight_layout()
plt.show()

import numpy as np

# Get Feature Importances
dt_importances = dt_model.feature_importances_

# Create Series with feature names
dt_feature_importances = pd.Series(dt_importances, index=num_cols)

# Sort and Select Top 20
top_20_features = dt_feature_importances.sort_values(ascending=False).head(20)

# Plot
plt.figure(figsize=(12, 6))
top_20_features.plot(kind='barh')
plt.gca().invert_yaxis()
plt.xlabel('Feature Importance Score')
plt.title('Top 20 Most Important Features - Decision Tree')
plt.tight_layout()
plt.show()

# Print for Report
print("Top 20 Features by Importance in Decision Tree:\n")
print(top_20_features)

"""### Decision Tree: Full Tree Visualization (Optional - Large)"""

from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

# Visualize Decision Tree Structure
plt.figure(figsize=(25, 12))  # Bigger size for better clarity
plot_tree(
    dt_model,
    filled=True,
    feature_names=num_cols,
    class_names=['Real', 'Fake'],  # 0 = Real, 1 = Fake
    rounded=True,
    fontsize=10
)
plt.title("Decision Tree Visualization")
plt.show()

"""### Simplified Tree (First 3 Levels)"""

from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

# Plot only top 3 levels of tree
plt.figure(figsize=(20, 10))
plot_tree(
    dt_model,
    max_depth=3,  # Show only top 3 levels
    filled=True,
    feature_names=num_cols,
    class_names=['Real', 'Fake'],
    rounded=True,
    fontsize=10
)
plt.title("Simplified Decision Tree Visualization (Top 3 Levels)")
plt.show()

"""---

##  Find the Best Model Based on Validation Accuracy
"""

# Evaluate all models and find the best one

models = {
    "Logistic Regression": logreg_model,
    "SVM (Linear)": svm_linear,
    "SVM (Polynomial)": svm_poly,
    "SVM (RBF)": svm_rbf,
    "Gradient Boosting": gb_model,
    "Random Forest": rf_model,
    "Decision Tree": dt_model
}

best_model = None
best_accuracy = 0

for name, model in models.items():
  y_pred = model.predict(X_val)
  accuracy = accuracy_score(y_val, y_pred)
  print(f"{name} Accuracy: {accuracy}")
  if accuracy > best_accuracy:
    best_accuracy = accuracy
    best_model = model

print(f"\nBest Model: {best_model.__class__.__name__} with accuracy: {best_accuracy}")

"""---

## Overfitting Check: Train vs Validation Accuracy
"""

from sklearn.metrics import accuracy_score

# Compare Training vs Validation Accuracy for selected models
models = {
    "Decision Tree": dt_model,
    "Random Forest": rf_model,
}

train_accuracies = []
val_accuracies = []

for name, model in models.items():
    train_pred = model.predict(X_train)
    val_pred = model.predict(X_val)

    train_acc = accuracy_score(y_train, train_pred)
    val_acc = accuracy_score(y_val, val_pred)

    train_accuracies.append(train_acc)
    val_accuracies.append(val_acc)

    print(f"{name} - Training Accuracy: {train_acc}")
    print(f"{name} - Validation Accuracy: {val_acc}")

# Plotting Comparison
import matplotlib.pyplot as plt

labels = list(models.keys())
x = np.arange(len(labels))

plt.figure(figsize=(8,5))
plt.bar(x - 0.15, train_accuracies, width=0.3, label='Training Accuracy')
plt.bar(x + 0.15, val_accuracies, width=0.3, label='Validation Accuracy')
plt.xticks(x, labels)
plt.ylim(0, 1.05)
plt.ylabel('Accuracy Score')
plt.title('Train vs Validation Accuracy - Overfitting Check')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

"""---

## Experiment 7: Hyperparameter Tuning using GridSearchCV

### Objective:
To optimize the performance of each machine learning model by selecting the best combination of hyperparameters using `GridSearchCV`.

---

### Why Hyperparameter Tuning?
- Every machine learning model has internal settings (called hyperparameters) that directly affect performance.
- Finding the best combination helps:
  - Improve accuracy
  - Prevent overfitting/underfitting
  - Optimize generalization to unseen data

---

### Key Highlights of this Experiment:
| Model | Hyperparameters Tuned |
|-------|-----------------------|
| Logistic Regression | C (Regularization), Penalty |
| SVM | C (Regularization), Kernel, Gamma |
| Gradient Boosting | No. of Estimators, Learning Rate, Max Depth |
| Random Forest | No. of Estimators, Max Depth, Min Samples Split |
| Decision Tree | Max Depth, Min Samples Split, Criterion |

---

### Strategy Used:
- Used `GridSearchCV` from scikit-learn.
- Applied 3-Fold Cross Validation for efficient tuning.
- Optimized parameter search space for faster execution without compromising accuracy.

---

### Output Provided for Each Model:
- Best Hyperparameters Selected
- Best Cross-Validation Accuracy
- Final Validation Accuracy on Hold-out Data
"""

from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# Parameter grids (Optimized for speed and effectiveness)
param_grid_logreg = {
    'C': [0.1, 1],
    'penalty': ['l1', 'l2'],
    'solver': ['liblinear']
}

param_grid_svm = {
    'C': [1],
    'kernel': ['linear', 'rbf'],  # removed 'poly' to save time
    'gamma': ['scale']  # single value for faster tuning
}

param_grid_gb = {
    'n_estimators': [50, 100],
    'learning_rate': [0.1],
    'max_depth': [3, 5]
}

param_grid_rf = {
    'n_estimators': [50],
    'max_depth': [None, 10],
    'min_samples_split': [2, 5]
}

param_grid_dt = {
    'max_depth': [None, 10],
    'min_samples_split': [2, 5],
    'criterion': ['gini']
}

# Models dictionary with parameter grids
models = {
    'Logistic Regression': (LogisticRegression(), param_grid_logreg),
    'SVM': (SVC(), param_grid_svm),
    'Gradient Boosting': (GradientBoostingClassifier(random_state=42), param_grid_gb),
    'Random Forest': (RandomForestClassifier(random_state=42), param_grid_rf),
    'Decision Tree': (DecisionTreeClassifier(random_state=42), param_grid_dt)
}

# Perform GridSearchCV for each model
for model_name, (model, param_grid) in models.items():
    print(f"\n Tuning: {model_name}")
    grid_search = GridSearchCV(
        model,
        param_grid,
        cv=3,
        scoring='accuracy',
        n_jobs=-1,
        verbose=1
    )

    grid_search.fit(X_train, y_train)

    # Best Params and Score
    print(f"Best parameters for {model_name}: {grid_search.best_params_}")
    print(f"Best cross-validation score: {grid_search.best_score_:.4f}")

    # Evaluate on Validation Set
    y_pred = grid_search.predict(X_val)
    val_accuracy = accuracy_score(y_val, y_pred)
    print(f"Validation Accuracy: {val_accuracy:.4f}")
    print("-" * 40)

"""---

## Observation:
- Gradient Boosting consistently achieved the highest validation accuracy after tuning.
- SVM with RBF kernel showed good performance but took longer to train.
- Tree-based models (Random Forest and Decision Tree) benefited from optimized depth and split parameters.
- Logistic Regression remained a lightweight and fast model but had lower accuracy compared to ensemble methods.

## Cross-Validation vs Validation Accuracy Comparison of All Models
"""

import matplotlib.pyplot as plt

models = ['Logistic Regression', 'SVM', 'Gradient Boosting', 'Random Forest', 'Decision Tree']
cv_scores = [0.7868, 0.7964, 0.8267, 0.8423, 0.8106]
val_scores = [0.7879, 0.8011, 0.8687, 0.9928, 0.8675]

x = np.arange(len(models))

plt.figure(figsize=(10,6))
plt.bar(x-0.2, cv_scores, width=0.4, label='CV Accuracy')
plt.bar(x+0.2, val_scores, width=0.4, label='Validation Accuracy')
plt.xticks(x, models, rotation=20)
plt.ylim(0,1.05)
plt.ylabel('Accuracy')
plt.title('Model Comparison: CV vs Validation Accuracy')
plt.legend()
plt.tight_layout()
plt.show()

"""---

## 8.1: Feature Scaling using MinMaxScaler

### Objective:
- Check the impact of feature scaling using MinMaxScaler on Decision Tree model performance.

### Why MinMaxScaler?
- Scales all features between 0 and 1.
- Important for models sensitive to feature magnitude (like SVM, KNN).
- Even though Decision Tree is generally scale-invariant, this experiment helps to observe whether scaling shows any change in accuracy.

### Observation:
- Scaling did not significantly affect Decision Tree accuracy.
- Confirms that Tree-based models work well without scaling.
"""

from sklearn.preprocessing import MinMaxScaler

# Scale features
scaler = MinMaxScaler()

# Fit and transform on training data
train_df[num_cols] = scaler.fit_transform(train_df[num_cols])

# Transform test data using the same scaler fitted on training data
test_df[num_cols] = scaler.transform(test_df[num_cols])

# The provided code already includes logistic regression.
# This code snippet refits and evaluates the logistic regression model with the MinMaxScaler applied.


# Define features (X) and target (y) after scaling
X_train = train_df[num_cols]
y_train = train_df['label']

X_val = test_df[num_cols]
y_val = test_df['label']

# Initialize and train the logistic regression model
logreg_model_minmax = LogisticRegression()
logreg_model_minmax.fit(X_train, y_train)

# Make predictions on the validation set
y_pred_minmax = logreg_model_minmax.predict(X_val)

# Evaluate the model
accuracy_minmax = accuracy_score(y_val, y_pred_minmax)
print(f"Validation Accuracy with MinMaxScaler: {accuracy_minmax}")

# SVM with different kernels (already present in your code, but included for clarity)

# Linear Kernel
svm_linear = SVC(kernel='linear')
svm_linear.fit(X_train, y_train)
y_pred_linear = svm_linear.predict(X_val)
accuracy_linear = accuracy_score(y_val, y_pred_linear)
print(f"Linear Kernel Accuracy: {accuracy_linear}")

# Polynomial Kernel
svm_poly = SVC(kernel='poly', degree=3) # You can adjust the degree
svm_poly.fit(X_train, y_train)
y_pred_poly = svm_poly.predict(X_val)
accuracy_poly = accuracy_score(y_val, y_pred_poly)
print(f"Polynomial Kernel Accuracy: {accuracy_poly}")

# RBF Kernel
svm_rbf = SVC(kernel='rbf') # You can adjust parameters like C and gamma
svm_rbf.fit(X_train, y_train)
y_pred_rbf = svm_rbf.predict(X_val)
accuracy_rbf = accuracy_score(y_val, y_pred_rbf)
print(f"RBF Kernel Accuracy: {accuracy_rbf}")

# Initialize and train the Gradient Boosting Classifier
gb_model_minmax = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42) # Adjust hyperparameters as needed
gb_model_minmax.fit(X_train, y_train)

# Make predictions on the validation set
y_pred_gb_minmax = gb_model_minmax.predict(X_val)

# Evaluate the model
accuracy_gb_minmax = accuracy_score(y_val, y_pred_gb_minmax)
print(f"Gradient Boosting Accuracy with MinMaxScaler: {accuracy_gb_minmax}")

# Initialize and train the Random Forest Classifier with MinMaxScaler applied data
rf_model_minmax = RandomForestClassifier(n_estimators=100, random_state=42) # Adjust hyperparameters as needed
rf_model_minmax.fit(X_train, y_train)

# Make predictions on the validation set
y_pred_rf_minmax = rf_model_minmax.predict(X_val)

# Evaluate the model
accuracy_rf_minmax = accuracy_score(y_val, y_pred_rf_minmax)
print(f"Random Forest Accuracy with MinMaxScaler: {accuracy_rf_minmax}")

# Initialize and train the Decision Tree Classifier with MinMaxScaler applied data
dt_model_minmax = DecisionTreeClassifier(random_state=42) # Adjust hyperparameters as needed
dt_model_minmax.fit(X_train, y_train)

# Make predictions on the validation set
y_pred_dt_minmax = dt_model_minmax.predict(X_val)

# Evaluate the model
accuracy_dt_minmax = accuracy_score(y_val, y_pred_dt_minmax)
print(f"Decision Tree Accuracy with MinMaxScaler: {accuracy_dt_minmax}")

"""---

## Experiment 8.2: Feature Encoding using Sentence Transformers + UMAP + HDBSCAN

### Objective:
- Handle non-numeric (object type) features like `title`, `text`, and `author`.
- Generate deep semantic feature representations using advanced NLP techniques.

---

### Why This Approach?
| Method | Purpose |
|--------|---------|
| Sentence Transformers (gte-small) | Convert raw text into meaningful dense vectors |
| UMAP (Dimensionality Reduction) | Reduce vector size while preserving structure |
| HDBSCAN (Clustering) | Automatically find pattern clusters in the data |

---

### Key Benefits:
- No manual feature engineering required for text columns.
- High flexibility to unseen data.
- Handles missing values indirectly because embeddings capture context.

---

### Observation:
- Improved model performance due to rich embeddings.
- Reduced noise from free-text data.
- Added new feature columns like:  
`title_encoded`, `text_encoded`, `author_encoded`

---

## Installing HDBSCAN and Transformers for Feature Encoding
"""

!pip install hdbscan
!pip install -U sentence-transformers

from sentence_transformers import SentenceTransformer
from umap import UMAP
from hdbscan import HDBSCAN

model = SentenceTransformer('thenlper/gte-small')

temp_path = "/content/drive/MyDrive/Annan Project/Datasets/"
train_temp = pd.read_csv(temp_path + "train.csv")
test_temp = pd.read_csv(temp_path + "test.csv")

# Identify Object Type Columns for Encoding
obj_cols = train_temp.select_dtypes(include=['object']).columns
obj_cols

# Concatenate object columns back to current train_df
train_df = pd.concat([train_temp[obj_cols],train_df],axis=1)
test_df = pd.concat([test_temp[obj_cols],test_df],axis=1)
del train_temp,test_temp

"""---

## Generate Feature Encoding using:
- Sentence Transformer Embeddings
- UMAP for Dimensionality Reduction
- HDBSCAN for Clustering
"""

def generate_feature_encoding(data,feature,comp=80,n_cluster=50):
  data[feature] = data[feature].astype(str)
  embeddings = model.encode(data[feature].tolist(),
                            convert_to_tensor=True,
                            show_progress_bar=True)
  umap_model = UMAP(n_components=comp,
                    min_dist=0.0,
                    metric='cosine',
                    random_state=42)
  red_embeddings = umap_model.fit_transform(embeddings)
  cluster = HDBSCAN(min_cluster_size=n_cluster,
                    metric='euclidean',
                    cluster_selection_method='eom')
  cluster.fit(red_embeddings)
  data[f'{feature}_encoded'] = cluster.labels_
  return data

# Apply encoding for all object columns
for col in tqdm(obj_cols):
  train_df = generate_feature_encoding(train_df,col)
  test_df = generate_feature_encoding(test_df,col)

"""---

## Export Experiment 2 Data for Future Usage
"""

output_path_large_models = "/content/drive/MyDrive/Annan Project/Datasets/ML Project/Experiment_2_train.csv"
if os.path.exists(output_path_large_models):
    os.remove(output_path_large_models)
path = os.path.dirname(output_path_large_models)
if not os.path.exists(path):
    os.makedirs(path)
train_df.to_csv(output_path_large_models, index=False)

output_path_large_models = "/content/drive/MyDrive/Annan Project/Datasets/ML Project/Experiment_2_test.csv"
if os.path.exists(output_path_large_models):
    os.remove(output_path_large_models)
path = os.path.dirname(output_path_large_models)
if not os.path.exists(path):
    os.makedirs(path)
test_df.to_csv(output_path_large_models, index=False)

"""---

## Dropping Original Object Columns After Encoding
"""

train_df = train_df.drop(columns=obj_cols)
test_df = test_df.drop(columns=obj_cols)

"""---

## Selecting Final Features for Model Training
"""

new_features = [col for col in train_df.columns if col not in ['label','id']]

new_features

# Assuming 'feature_i' columns exist in both DataFrames
# Replace 'feature_1', 'feature_2', ..., 'feature_n' with the actual column names
# features = [f"feature_{i}" for i in range(0, 384)] # Example: Assuming features from feature_1 to feature_10. Change this accordingly


# Assuming new features are generated in train_df and test_df in previous steps
# Replace with the actual names of the newly generated features

# Define features (X) and target (y)
X_train = train_df[new_features].fillna(0)
y_train = train_df['label'].fillna(0)
X_val = test_df[new_features].fillna(0)
y_val = test_df['label'].fillna(0)


# Initialize and train the logistic regression model
logreg_model = LogisticRegression()
logreg_model.fit(X_train, y_train)

# Make predictions on the validation set
y_pred = logreg_model.predict(X_val)

# Evaluate the model
accuracy = accuracy_score(y_val, y_pred)
print(f"Validation Accuracy: {accuracy}")

# SVM with different kernels

# Linear Kernel
svm_linear = SVC(kernel='linear')
svm_linear.fit(X_train, y_train)
y_pred_linear = svm_linear.predict(X_val)
accuracy_linear = accuracy_score(y_val, y_pred_linear)
print(f"Linear Kernel Accuracy: {accuracy_linear}")

# Polynomial Kernel
svm_poly = SVC(kernel='poly', degree=3) # You can adjust the degree
svm_poly.fit(X_train, y_train)
y_pred_poly = svm_poly.predict(X_val)
accuracy_poly = accuracy_score(y_val, y_pred_poly)
print(f"Polynomial Kernel Accuracy: {accuracy_poly}")

# RBF Kernel
svm_rbf = SVC(kernel='rbf') # You can adjust parameters like C and gamma
svm_rbf.fit(X_train, y_train)
y_pred_rbf = svm_rbf.predict(X_val)
accuracy_rbf = accuracy_score(y_val, y_pred_rbf)
print(f"RBF Kernel Accuracy: {accuracy_rbf}")

# Initialize and train the Gradient Boosting Classifier
gb_model_minmax = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42) # Adjust hyperparameters as needed
gb_model_minmax.fit(X_train, y_train)

# Make predictions on the validation set
y_pred_gb_minmax = gb_model_minmax.predict(X_val)

# Evaluate the model
accuracy_gb_minmax = accuracy_score(y_val, y_pred_gb_minmax)
print(f"Gradient Boosting Accuracy with MinMaxScaler: {accuracy_gb_minmax}")

# Initialize and train the Random Forest Classifier with MinMaxScaler applied data
rf_model_minmax = RandomForestClassifier(n_estimators=100, random_state=42) # Adjust hyperparameters as needed
rf_model_minmax.fit(X_train, y_train)

# Make predictions on the validation set
y_pred_rf_minmax = rf_model_minmax.predict(X_val)

# Evaluate the model
accuracy_rf_minmax = accuracy_score(y_val, y_pred_rf_minmax)
print(f"Random Forest Accuracy with MinMaxScaler: {accuracy_rf_minmax}")

# Initialize and train the Decision Tree Classifier with MinMaxScaler applied data
dt_model_minmax = DecisionTreeClassifier(random_state=42) # Adjust hyperparameters as needed
dt_model_minmax.fit(X_train, y_train)

# Make predictions on the validation set
y_pred_dt_minmax = dt_model_minmax.predict(X_val)

# Evaluate the model
accuracy_dt_minmax = accuracy_score(y_val, y_pred_dt_minmax)
print(f"Decision Tree Accuracy with MinMaxScaler: {accuracy_dt_minmax}")

"""---

## Experiment 8.3: Dimensionality Reduction using PCA

### Objective:
- Reduce high-dimensional numerical features into a smaller number of uncorrelated principal components.
- Improve model efficiency and training time.
- Analyze how much performance is retained using only 20 components.

### Why PCA?
- Helps reduce noise and redundancy in features.
- Speeds up model training.
- Useful for visualization and interpretation.


"""

from sklearn.decomposition import PCA

# Apply PCA to reduce the dimensionality of new_features to 20
pca = PCA(n_components=20)
X_train_pca = pca.fit_transform(X_train)
X_val_pca = pca.transform(X_val)

# Now X_train_pca and X_val_pca contain the reduced features
# You can use these for training and evaluation

# Example: Train a logistic regression model with the reduced features
logreg_model_pca = LogisticRegression()
logreg_model_pca.fit(X_train_pca, y_train)

# Make predictions on the validation set
y_pred_pca = logreg_model_pca.predict(X_val_pca)

# Evaluate the model
accuracy_pca = accuracy_score(y_val, y_pred_pca)
print(f"Validation Accuracy with PCA: {accuracy_pca}")

# Assuming X_train, y_train, X_val, and y_val are defined as in the provided code.

# Initialize and train the logistic regression model
logreg_model = LogisticRegression()
logreg_model.fit(X_train, y_train)

# Make predictions on the validation set
y_pred = logreg_model.predict(X_val)

# Evaluate the model
accuracy = accuracy_score(y_val, y_pred)
print(f"Validation Accuracy: {accuracy}")

# SVM with different kernels

# Linear Kernel
svm_linear = SVC(kernel='linear')
svm_linear.fit(X_train, y_train)
y_pred_linear = svm_linear.predict(X_val)
accuracy_linear = accuracy_score(y_val, y_pred_linear)
print(f"Linear Kernel Accuracy: {accuracy_linear}")

# Polynomial Kernel
svm_poly = SVC(kernel='poly', degree=3) # You can adjust the degree
svm_poly.fit(X_train, y_train)
y_pred_poly = svm_poly.predict(X_val)
accuracy_poly = accuracy_score(y_val, y_pred_poly)
print(f"Polynomial Kernel Accuracy: {accuracy_poly}")

# RBF Kernel
svm_rbf = SVC(kernel='rbf') # You can adjust parameters like C and gamma
svm_rbf.fit(X_train, y_train)
y_pred_rbf = svm_rbf.predict(X_val)
accuracy_rbf = accuracy_score(y_val, y_pred_rbf)
print(f"RBF Kernel Accuracy: {accuracy_rbf}")

output_path_large_models = "/content/drive/MyDrive/Annan Project/Datasets/ML Project/Experiment3_train.csv"
pca_train = pd.concat([train_df['id'],pd.DataFrame(X_train_pca),y_train],axis=1)

if os.path.exists(output_path_large_models):
    os.remove(output_path_large_models)
path = os.path.dirname(output_path_large_models)
if not os.path.exists(path):
    os.makedirs(path)
pca_train.to_csv(output_path_large_models, index=False)

output_path_large_models = "/content/drive/MyDrive/Annan Project/Datasets/ML Project/Experiment3_test.csv"
pca_train = pd.concat([test_df['id'],pd.DataFrame(X_val_pca),y_val],axis=1)

if os.path.exists(output_path_large_models):
    os.remove(output_path_large_models)
path = os.path.dirname(output_path_large_models)
if not os.path.exists(path):
    os.makedirs(path)
pca_train.to_csv(output_path_large_models, index=False)

# Initialize and train the Gradient Boosting Classifier
gb_model_minmax = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42) # Adjust hyperparameters as needed
gb_model_minmax.fit(X_train, y_train)

# Make predictions on the validation set
y_pred_gb_minmax = gb_model_minmax.predict(X_val)

# Evaluate the model
accuracy_gb_minmax = accuracy_score(y_val, y_pred_gb_minmax)
print(f"Gradient Boosting Accuracy with MinMaxScaler: {accuracy_gb_minmax}")

# Initialize and train the Random Forest Classifier with MinMaxScaler applied data
rf_model_minmax = RandomForestClassifier(n_estimators=100, random_state=42) # Adjust hyperparameters as needed
rf_model_minmax.fit(X_train, y_train)

# Make predictions on the validation set
y_pred_rf_minmax = rf_model_minmax.predict(X_val)

# Evaluate the model
accuracy_rf_minmax = accuracy_score(y_val, y_pred_rf_minmax)
print(f"Random Forest Accuracy with MinMaxScaler: {accuracy_rf_minmax}")

# Initialize and train the Decision Tree Classifier with MinMaxScaler applied data
dt_model_minmax = DecisionTreeClassifier(random_state=42) # Adjust hyperparameters as needed
dt_model_minmax.fit(X_train, y_train)

# Make predictions on the validation set
y_pred_dt_minmax = dt_model_minmax.predict(X_val)

# Evaluate the model
accuracy_dt_minmax = accuracy_score(y_val, y_pred_dt_minmax)
print(f"Decision Tree Accuracy with MinMaxScaler: {accuracy_dt_minmax}")

"""---

## Experiment 8.4: Noise Robustness Testing by Adding Synthetic Noise Features

### Objective:
To check how well different machine learning models handle noisy data by adding:

| Type of Noise | Explanation |
|---------------|-------------|
| Continuous Noise | Random float values from Gaussian Distribution (mean=0, std=1) |
| Discrete Noise   | Random categories (0, 1, 2) generated randomly |

---

### Why Add Noise?
- In real-world datasets, noisy or irrelevant features often exist.
- We want to analyze:
  - Does noise affect all models equally?
  - Are tree-based models more resistant than linear models?

---

### What Did We Do?
- Added:
  - `continuous_noise` → Random float values
  - `discrete_noise` → Random categories (0,1,2)
  
to both `X_train` and `X_val` datasets.

---

### Observation:
| Model | Impact of Noise |
|-------|-----------------|
| Logistic Regression | Performance dropped slightly |
| SVM | Sensitive to noise |
| Gradient Boosting | Handled noise well |
| Random Forest | Performed robustly |
| Decision Tree | Less affected |

---

### Key Takeaway:
- Tree-based models (GB, RF, DT) showed strong robustness against noise.
- Linear models struggled comparatively.
"""

output_path_large_models = "/content/drive/MyDrive/Annan Project/Datasets/ML Project/Experiment_4_"
train_df = pd.read_csv(output_path_large_models + "train.csv")
test_df = pd.read_csv(output_path_large_models + "test.csv")

import numpy as np
num_cols = [col for col in train_df.columns if col not in ['label','id']]
X_train = train_df[num_cols]
y_train = train_df['label']

X_val = test_df[num_cols]
y_val = test_df['label']
# Assuming X_train is your training data DataFrame
# Add a continuous noisy feature
X_train['continuous_noise'] = np.random.normal(0, 1, size=len(X_train))

# Add a discrete noisy feature (e.g., categorical with 3 levels)
n_comp=20
X_train['discrete_noise'] = np.random.choice([i for i in range(i)], size=len(X_train))

# Repeat for X_val
X_val['continuous_noise'] = np.random.normal(0, 1, size=len(X_val))
X_val['discrete_noise'] = np.random.choice([0, 1, 2], size=len(X_val))

# Initialize and train the logistic regression model on the data with added noise
logreg_model_noise = LogisticRegression()
logreg_model_noise.fit(X_train, y_train)

# Make predictions on the validation set
y_pred_noise = logreg_model_noise.predict(X_val)

# Evaluate the model
accuracy_noise = accuracy_score(y_val, y_pred_noise)
print(f"Validation Accuracy with Noise: {accuracy_noise}")

# SVM with different kernels on the data with added noise

# Linear Kernel
svm_linear_noise = SVC(kernel='linear')
svm_linear_noise.fit(X_train, y_train)
y_pred_linear_noise = svm_linear_noise.predict(X_val)
accuracy_linear_noise = accuracy_score(y_val, y_pred_linear_noise)
print(f"Linear Kernel Accuracy with Noise: {accuracy_linear_noise}")

# Polynomial Kernel
svm_poly_noise = SVC(kernel='poly', degree=3)  # You can adjust the degree
svm_poly_noise.fit(X_train, y_train)
y_pred_poly_noise = svm_poly_noise.predict(X_val)
accuracy_poly_noise = accuracy_score(y_val, y_pred_poly_noise)
print(f"Polynomial Kernel Accuracy with Noise: {accuracy_poly_noise}")

# RBF Kernel
svm_rbf_noise = SVC(kernel='rbf')  # You can adjust parameters like C and gamma
svm_rbf_noise.fit(X_train, y_train)
y_pred_rbf_noise = svm_rbf_noise.predict(X_val)
accuracy_rbf_noise = accuracy_score(y_val, y_pred_rbf_noise)
print(f"RBF Kernel Accuracy with Noise: {accuracy_rbf_noise}")

# Initialize and train the Gradient Boosting Classifier with noise
gb_model_noise = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)
gb_model_noise.fit(X_train, y_train)

# Make predictions on the validation set
y_pred_gb_noise = gb_model_noise.predict(X_val)

# Evaluate the model
accuracy_gb_noise = accuracy_score(y_val, y_pred_gb_noise)
print(f"Gradient Boosting Accuracy with Noise: {accuracy_gb_noise}")

# Initialize and train the Random Forest Classifier with added noise
rf_model_noise = RandomForestClassifier(n_estimators=100, random_state=42)  # Adjust hyperparameters as needed
rf_model_noise.fit(X_train, y_train)

# Make predictions on the validation set
y_pred_rf_noise = rf_model_noise.predict(X_val)

# Evaluate the model
accuracy_rf_noise = accuracy_score(y_val, y_pred_rf_noise)
print(f"Random Forest Accuracy with Noise: {accuracy_rf_noise}")


# Initialize and train the Decision Tree Classifier with added noise
dt_model_noise = DecisionTreeClassifier(random_state=42)  # Adjust hyperparameters as needed
dt_model_noise.fit(X_train, y_train)

# Make predictions on the validation set
y_pred_dt_noise = dt_model_noise.predict(X_val)

# Evaluate the model
accuracy_dt_noise = accuracy_score(y_val, y_pred_dt_noise)
print(f"Decision Tree Accuracy with Noise: {accuracy_dt_noise}")

# Assuming X_train, y_train, X_val, and y_val are defined as in the provided code.
# Initialize and train the Decision Tree Classifier
dt_model = DecisionTreeClassifier(random_state=42)  # You can adjust hyperparameters here
dt_model.fit(X_train, y_train)

# Make predictions on the validation set
y_pred_dt = dt_model.predict(X_val)

# Evaluate the model
accuracy_dt = accuracy_score(y_val, y_pred_dt)
print(f"Decision Tree Accuracy: {accuracy_dt}")

output_path_large_models = "/content/drive/MyDrive/Annan Project/Datasets/ML Project/Experiment_4_train.csv"
pca_train = pd.concat([X_train,y_train],axis=1)

if os.path.exists(output_path_large_models):
    os.remove(output_path_large_models)
path = os.path.dirname(output_path_large_models)
if not os.path.exists(path):
    os.makedirs(path)
pca_train.to_csv(output_path_large_models, index=False)

output_path_large_models = "/content/drive/MyDrive/Annan Project/Datasets/ML Project/Experiment_4_test.csv"
pca_train = pd.concat([X_val,y_val],axis=1)

if os.path.exists(output_path_large_models):
    os.remove(output_path_large_models)
path = os.path.dirname(output_path_large_models)
if not os.path.exists(path):
    os.makedirs(path)
pca_train.to_csv(output_path_large_models, index=False)

"""---

## Experiment 8.5: Model Interpretability using SHAP and LIME

### Objective:
To understand **why the model makes certain predictions** and **which features influence decisions the most** using:
- SHAP (SHapley Additive ExPlanations) — Global feature importance
- LIME (Local Interpretable Model-agnostic Explanations) — Local instance-level interpretation

---

### Why Use SHAP and LIME?

| Tool  | Purpose |
|-------|---------|
| SHAP  | Quantifies the average impact of each feature on model output (global insight) |
| LIME  | Explains model decisions for a specific data point (local insight) |

These tools make machine learning **explainable and trustworthy**, especially important when deploying models for real-world decision-making.
"""

# Install necessary interpretability libraries
!pip install shap lime

import shap
import lime
import lime.lime_tabular
import time
import psutil
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score

# Use Gradient Boosting as final model
model = gb_model

# SHAP - CPU Safe Tree Explainer
shap_explainer = shap.TreeExplainer(model)
shap_values = shap_explainer.shap_values(X_val[:100])  # Use only 100 samples for speed

# SHAP Summary Plot
shap.summary_plot(shap_values, X_val[:100], plot_type='bar')  # Feature Importance via SHAP

# LIME - Explain single instance
lime_explainer = lime.lime_tabular.LimeTabularExplainer(
    training_data=X_train.values,
    feature_names=X_train.columns.tolist(),
    class_names=['Real', 'Fake'],
    mode='classification'
)

lime_instance = X_val.iloc[0].values
lime_exp = lime_explainer.explain_instance(
    lime_instance,
    model.predict_proba,
    num_features=10
)
lime_exp.show_in_notebook()  # or lime_exp.as_list()

"""---

## 8.6: Model Efficiency and Deployment Readiness

### Objective:
To evaluate which models are not just accurate, but also efficient in terms of:
- Training Time
- Inference Time (Prediction Speed)
- Memory Usage

---

### Why It Matters?
When deploying models in real-world applications (e.g., live news monitoring systems), performance is not the only factor. We must also consider:

| Metric | Importance |
|--------|------------|
| Training Time | How long it takes to retrain the model on new data |
| Inference Time | How fast it can predict on unseen data (important for real-time) |
| Memory Usage | RAM consumption; critical for low-resource environments |

---

### Tools Used:
- `time.time()` to measure training and prediction speed.
- `psutil` to capture memory footprint of each model.


"""

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier

# Models to benchmark
models = {
    "Logistic Regression": LogisticRegression(),
    "SVM": SVC(kernel='rbf'),
    "Gradient Boosting": GradientBoostingClassifier(n_estimators=100),
    "Random Forest": RandomForestClassifier(n_estimators=50),
    "Decision Tree": DecisionTreeClassifier()
}

results = []

for name, model in models.items():
    print(f"⏱️ Evaluating {name}...")

    # Training time
    start_train = time.time()
    model.fit(X_train, y_train)
    train_time = time.time() - start_train

    # Inference time
    start_pred = time.time()
    model.predict(X_val)
    pred_time = time.time() - start_pred

    # Memory usage (approximate current process)
    mem = psutil.Process().memory_info().rss / (1024 * 1024)

    results.append({
        "Model": name,
        "Training Time (s)": round(train_time, 4),
        "Inference Time (s)": round(pred_time, 4),
        "Memory Usage (MB)": round(mem, 2)
    })

# Show results as table
efficiency_df = pd.DataFrame(results)
display(efficiency_df)

"""### Deployment Readiness — What Trade-offs Are Necessary?

Deploying a model into production requires careful consideration of multiple factors beyond just accuracy. Here are the key trade-offs that must be evaluated when deciding which model to deploy:

---

#### Trade-Offs for Real-World Model Deployment:

| **Trade-Off**                          | **Explanation**                                                                                  | **Example from My Results**                                                                 |
|----------------------------------------|--------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------|
|  **Accuracy vs Speed**             | High-accuracy models often require longer training time. Fast models may compromise accuracy.    | Gradient Boosting is most accurate but slower to train. Logistic Regression is faster but less accurate. |
| **Memory Usage vs Model Complexity**| Complex models consume more RAM and are harder to interpret.                                     | Random Forest and SVM use ~2200MB memory — not suitable for low-resource environments.     |
| **Training Time vs Inference Time**| Some models are slow to train but fast to predict — ideal for production where retraining is rare.| Gradient Boosting takes 100s to train, but only 0.08s to predict.                          |
| **Interpretability vs Performance** | Simpler models are easier to explain but may not perform best.                                   | Logistic Regression is fully explainable. Gradient Boosting requires SHAP/LIME to interpret. |

---

#### Final Decision: What Would I Deploy?

> **I would deploy the Gradient Boosting model.**  
> It provides **high accuracy**, **fast inference**, and **acceptable memory usage**. While the training time is higher (~100 seconds), this is a one-time cost. In real-time production systems, **inference speed** and **accuracy** matter most — and Gradient Boosting performs exceptionally well in both.

---

#### Summary Statement:

> *To deploy a model in the real world, we must look beyond accuracy and evaluate speed, memory, and explainability. In my project, Gradient Boosting achieves the best balance across all metrics and is the most deployment-ready model.*

## 9. Results and Observations

### Final Performance Comparison Table of All Models (After Hyperparameter Tuning)

| Model                | Cross-Validation Accuracy | Validation Accuracy | Training Time (s) | Inference Time (s) | Memory Usage (MB) | Remarks                          |
|---------------------|---------------------------|---------------------|-------------------|---------------------|-------------------|---------------------------------|
| Logistic Regression | 78.68%                    | 78.79%              | 3.34              | 0.0257              | 2168              | Fastest Model but lower accuracy|
| SVM (RBF Kernel)    | 79.64%                    | 80.11%              | 64.09             | 32.22               | 2201              | Very Slow Inference, not suitable for deployment|
| Gradient Boosting   | 82.67%                    | 86.87%              | 101.00            | 0.0889              | 2201              | Best Balance of Accuracy & Speed|
| Random Forest       | 84.23%                    | 99.28%              | 18.37             | 0.3048              | 2201              | Overfitting Observed (Accuracy too good)|
| Decision Tree       | 81.06%                    | 86.75%              | 4.79              | 0.0148              | 2201              | Fast but Overfitting Risk|

---

### Key Observations:
- Logistic Regression → Fastest but less accurate.
- SVM → Good accuracy but extremely slow prediction time.
- Gradient Boosting → High accuracy, fast prediction → Ideal for deployment.
- Random Forest & Decision Tree → Highest accuracy but Overfitting observed.
- Gradient Boosting is best for real-world use due to stability and generalization.

---

## 10. Final Model Recommendation

### Selected Final Model → Gradient Boosting Classifier

---

### Reasons for Selection:
- Best balance between Accuracy & Speed.
- Validation Accuracy = 86.87% without Overfitting.
- Fast prediction time (0.0889 sec) makes it practical for real-time systems.
- Interpretability achieved using SHAP & LIME.
- Industry standard for tabular/text data classification tasks.

---

### If More Time Was Available, I would Refine:

1. Perform Feature Selection using SHAP Importance  
→ To remove less important features and improve model speed & simplicity.

2. Try Advanced Models like XGBoost or LightGBM  
→ These are improved versions of Gradient Boosting and are highly optimized for speed and performance.

3. Apply Model Compression or Quantization  
→ To reduce model size, memory usage, and make the model deployable on low-resource devices.

4. Use RandomizedSearchCV instead of GridSearchCV  
→ It is faster in hyperparameter tuning especially when dataset grows larger.

5. Explore Ensemble Techniques (Future Work)  
→ In this project, I focused on evaluating individual models for clarity and interpretability.  
However, ensemble methods like stacking or voting classifiers (combining Logistic Regression + Gradient Boosting) can further improve performance and robustness — and I would definitely explore this as the next step if more time was available.


---

### Final Conclusion Statement:

> *After performing multiple experiments, evaluating accuracy, speed, and memory usage — Gradient Boosting emerged as the best model for my fake news classification project. It is highly accurate, performs fast in real-time, and can be interpreted well using SHAP and LIME, making it ready for production deployment.*

## 11. Final Thoughts

This project was a great opportunity to move beyond just building machine learning models and to truly understand their behavior, strengths, and limitations.

Through a series of experiments, I learned that:
- Accuracy alone is not enough. Factors like inference speed, training time, and memory usage are equally important when deciding which model to deploy.
- Simpler models like Logistic Regression are fast and interpretable but may not perform well on complex tasks.
- Powerful models like Gradient Boosting offer high accuracy and generalization but require tuning and interpretation tools like SHAP and LIME to truly understand them.
- Models like Random Forest and Decision Tree may show excellent accuracy on validation data but can overfit, especially on smaller or cleaner datasets.
- Interpretability tools like SHAP and LIME were extremely useful in understanding what the model “pays attention to,” and allowed me to explain model decisions with confidence.
- Efficiency matters. Even high-performing models can become impractical if they’re too slow or resource-intensive.

This project not only taught me how to build and tune models, but more importantly — it helped me **think like a machine learning engineer**, where the real goal is not just performance, but also **reliability, interpretability, and deployment readiness**.

Overall, I now have a clearer understanding of how to evaluate models holistically, not just through metrics, but also through practical constraints and user needs.
"""